---
title: "Microbiome Training Module - BD2K 2018"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
  word_document: default
---

***

***

# Microbiome Training Module 

***

***








***

## Part 1: BASH Review 

### *A. Review file structure navigation with the command line*


As shown in Figure 1 below, files are contained inside directories (aka folders), which may themselves be nested within each other. We are used to opening directories in series to find a file we are looking for. On the command line, we can similarly navigate the tree structure using the “cd” command and “/” to separate directory and file names.

**Figure 1: File system structure looks like an upside down tree with branches** 
![](~/microbiome_module/training_exercise/scripts/Figure_1.png)

1.	The text that precedes the $ in your terminal indicates the directory that you are currently in. You will start in your home directory (represented by the tilde ~)

2.	Navigate to a directory named “directory_practice” for practice moving up and down the tree structure. The command “pwd” will print the directory that you are currently in, which is very useful if you get lost! 


```{bash}
# start in home directory and print the working directory to the screen
cd ~
pwd

#navigate to directory practice
cd microbiome_module/directory_practice
```

3.	To visualize the entire “tree” structure at once, use the tree command. *Which subdirectories(s) have no files inside? Which subdirectory has only one text file inside?*

```{bash}
# visualize the directory and sub-directories as a tree
tree
```

4.	We can navigate directory to a directory by typing in the exact address – this is akin to telling someone that JAX is at 10 Discovery Drive, Farmington CT, 06032, USA. Or you can navigate by giving relative directions, using a series of “cd” operations to move down the tree structure. Note that “.” Represents your current directory and “..” represents a parent directory, which can be useful for moving up the tree structure as in “cd ..” *Navigate directory to directory S3. What directory do you end up in if you then move up the tree structure by one directory?* 

```{bash}
# change directory to S3 using its exact address
cd B/S3

cd ..
```

5.	We can use the “ls” command to list the contents of a directory, either additional directories or files.  Flags can be appended to Linux commands to reveal more information. You can find out more information about a Linux command and the allowable flags with the “-- help” flag. *Navigate to the directory S4. What are the names of the files inside? What happens if you add the -a flag to the ls command?*

```{bash}
# change directory to S4
cd S4

# list the contents of the directory
ls
```

6.	We can add to the tree structure by making new directories. *Make a new directory within “directory_practice/A/S1” called “newdir” using the “mkdir” command. Visualize the new tree structure for “directory_practice”.*

```{bash}
# change directory to S1
cd ~/microbiome_module/directory_practice/A/S1

# list the current contents of S1
ls

# make a new directory with the name "newdir"
mkdir newdir

# now list the current contents of S1. See "newdir is now there.
ls

# go back to the top level directory
cd ~/microbiome_module/directory_practice

# visualize the directory and sub-directories as a tree
tree
```

***














***

### *B. Review key BASH commands*

***

Bash is a powerful tool for not only storing and navigating files and directories, but also peering inside of them and manipulating them. We will be using several LINUX commands to handle 16S sequencing data. Here, let’s practice using them with non-sequencing examples and then we can apply them to 16S later in the module. 

*First, navigate to directory S4 within the B directory within the “directory_practice” directory.*

1.	**head**. Head is a convenient tool for getting a quick glimpse at the contents of a file without having to visualize the entire document. Head displays the first 10 lines of a file to the screen. There are additional flags for customizing the output of head. For example, “head -n” will print the first “n” number of lines. See “head -- help” for details. *What happens when you issue the command “head JAX.txt”? How about “head -3 JAX.txt”?*

```{bash}
# print your current working directory
pwd

# navigate to directory S4
cd ~/microbiome_module/directory_practice/B/S4

# look at the first three lines of the file JAX.txt
head -3 JAX.txt

```

***

2.	**cat**. Cat is an extremely useful command that will concatenate files or standard input to standard output. We will be using the cat command to pool sequences together from different samples. Use the “>” character to define the standard output from the cat command. *Look at microbiome.txt. Combine microbiome.txt with JAX.txt and save the output as a new file microbiomeJAX.txt. Use head to look at the first 8 lines of the new file.*

```{bash}
# look at file microbiome.txt. NOTE that head displays the first 10 lines by default unless otherwise specified
head microbiome.txt

# concatenate two text files using "cat" and then output the result using ">" to a new file, called microbiomeKJAX.txt
cat microbiome.txt JAX.txt > microbiomeJAX.txt

# look at the first 8 lines of the new file. See that they were concatenated.
head -8 microbiomeJAX.txt

```

***

3.	**grep**. Grep is a LINUX command for finding instances of patterns within a file. It is particularly useful in combination with other commands, but first explore grep on its own. *Look again at the microbiome.txt file. Use grep to return any line that has the letter “f” in it. Which lines have the letter “s”?*

```{bash}
# look at the file microbiome.txt
head microbiome.txt

# use "grep" to find lines with the character "f" in the file microbiome.txt
grep "f" microbiome.txt

# use "grep" to find lines with the character "f" in the file microbiome.txt
grep "s" microbiome.txt

```

***

4.	**wc**. Wc is a counting tool typically used in combination with a flag. Depending on the flag, wc can be used to count the number of characters (-m), line (-l), words (-w), etc in a file. See “wc – help” for details. *How many lines are in microbiome.txt? How many words?*

```{bash}
# use "wc" with the "-l" flag to count the number of lines in the file microbiome.txt
wc -l microbiome.txt

# use "wc" with the "-w" flag to count the number of words in the file microbiome.txt
wc -w microbiome.txt

```

***

5.	**|**The “|” character is called a “pipe,” and it allows you to send the output from one command as the input to another command, as if the two commands were connected by a pipe. With the “|” command, we can combine simple LINUX commands to achieve complex outcomes. *Use grep in combination with wc to count the number of lines in microbiome.txt that contain the “>” character. Then output any line that contains “fun” in JAX.txt or microbiome.txt to a new file, called fun.txt. Look at the contents of fun.txt. Did your code work?*

```{bash}
# first use "grep" to identify lines with the ">" character and then pass the result with a pipe "|" to wc -l" to count the number of lines
grep ">" microbiome.txt | wc -l

# first use "cat" to join two files and then pass the result with a pipe "|" to "grep" and look for the word "fun" and send the result to a file called "fun.txt"
cat microbiome.txt JAX.txt | grep "fun" > fun.txt

# look at the file "fun.txt" See how the code above worked.
head fun.txt

```

***

















***

### *C. "Writing For loops in Bash"*

In any computing language, “for loops” are an essential approach for repeating a section of code over and over again while a certain condition (for ...) is met. Once the condition has been satisfied, the for loop concludes. This is particularly useful to us because we have 30 pair-end read files and 15 different patient samples, each of which contains many many individual sequencing reads, and we want to treat and process all of these samples and reads equally. For our purposes, we will use for loops to apply some operation again and again until all patient samples have been analyzed.

```{bash}
for item in list_of_items
  do operation(s) to be executed on each item
done
  
```

This for loop will chug through a list of items one by item and apply the same set of operation(s) on each and every item. Note that the “$” character is used to refer to a previously defined variable that can take on different values as the for loop progresses. 

let’s write a for loop to write the filename and count the number of lines in every text file in directory S4. Note that "$*$" is a wildtype character, so "$*$.txt” means any file in the directory that ends with .txt”. We can write this cleanly on one line by separating statements with semi-colons.

```{bash}
# show all .txt files in the current working directory (~/microbiome_module/directory_practice/B/S4)
ls *.txt

# we are going to iterate over all txt files in the working directory
# for each file the variable file takes on, count the number of lines
# end the for loop

for file in *.txt
  do wc -l $file
done

```










***

## Part 2: 16S Microbiome analysis

***



** ZOOM OUT TO OVERVIEW **




###*A sourcing and uncompressing files

The goal of this cohort is to compare microbiome in infants who have developed type 1 diabetes (T1D) or serum autoantibodies (markers predicting the onset of T1D) with healthy controls in the same area.

[DIAIMMUNE T1D Cohort](https://pubs.broadinstitute.org/diabimmune/t1d-cohort)

We can find the 16S reads on this webpage:
[16S reads](https://pubs.broadinstitute.org/diabimmune/t1d-cohort/resources/16s-sequence-data)

Let's look at the metadata to see who these patients are.

**NOW SWITCH OVER TO "CONSOLE." WE WILL BE USING "R."**

```{r}
# set your working directory so R knows where to look for files
setwd("~/microbiome_module/training_exercise/resources")

# read the csv file we've downloaded from the DIABIMMUNE webpage into R
df_meta_full<-read.table('diabimmune_metadata_full.csv', header=TRUE, sep=',', row.names=1)

# look at just the first 8 columns and first 5 rows
head(df_meta_full[,1:8])

# look at the whole table, which will wrap
head(df_meta_full)
```

*The first thing to do is to decide which patients you want to study. This is an opportunity for students to choose their own adventure.* I've chosen to focus on just the last collection timepoint for each of the 33 patients enrolled in the study. Of course, one could also look at a different cohort or look at this longitudinally, etc. Consider that the more patients you look at, the more time it will take to computationally process the data.

```{r}
# read the csv file with the 33 samples of interest
df_meta<-read.table('metadata_last_collection.csv', header=TRUE, sep=',', row.names=1)

# now look at all 33 patients, but just the first 8 rows
df_meta[,1:8]

# look at all the rows, which will wrap (note that R caps what you print)
head(df_meta)
```

So we know we want these patients. We could of course download files 1 by 1, but it gets a bit tedious! Once could also use the command wget to download files programmatically. One would follow these instructions. *There are thousands of available sequences- careful!*

*Download wget from GNU's FTP server or install using tools such as Homebrew, apt-get or similar command-line package utilites.
*Open a console window on your machine, and navigate to the directory where you would like to download the data.
*Run the following command: wget -r -np -nd https://pubs.broadinstitute.org/diabimmune/data/9

*NOTE: For the purposes of this exercise, the files have already been downloaded for you*















***


**NOW SWITCH OVER TO "TERMINAL."**


***

First, let's navigate through the directory, which is called "training_exercise" to look at what we've got.

```{bash}
cd ~/microbiome_module/training_exercise
ls
```

In this directory you will find:

*scripts: here I've put some useful scripts that will automate some of our most tedious processes in handling 16S sequences
*resources: this contains our metadata spreadsheets
*fastqs: this contains the raw fastq files I've downloaded from DIABIMMUNE
*backup_files: just in case someone deletes their files by accident
*example_files: completed analysis in case we get stuck

*First, let's look at the raw fastq files*

```{bash}
cd ~/microbiome_module/training_exercise/fastqs
ls
```

You will notice that these are all red! That means that they are compressed files. Note their suffix ".gz" Before we do anything, we first need to uncompress these files. We can write a "for loop" to go through all the files automatically.


```{bash}
# these files are in compressed format. Let's uncompress them using gunzip
# for each fasta.gz file, uncompress it
for file in *.fastq.gz
do
gunzip $file
done
```

*Did all of your sequences turn black? Use *ls* to look at the files in the directory*

***





** ZOOM OUT TO OVERVIEW **






###*B Renaming files and creating symbolic links

**CONTINUE WORKING IN YOUR "TERMINAL."**

All of our files look the same, but we want to rename which are cases and which are controls, so we can keep track. This will be useful at the end when we want to visualize the sequences in R. The following code will relabel the control patient sequence names. 

```{bash}
while read line
do
READ1="${line}_R1_001.fastq"
READ2="${line}_R2_001.fastq"
CONDITION='control_'
RENAME=$CONDITION$line
READ1RENAME="${RENAME}_R1_001.fastq"
READ2RENAME="${RENAME}_R2_001.fastq"
mv $READ1 $READ1RENAME
mv $READ2 $READ2RENAME

done < ~/microbiome_module/training_exercise/resources/patient_control.txt
```

Now relabel the case patient sequence names as well

```{bash}
while read line
do
READ1="${line}_R1_001.fastq"
READ2="${line}_R2_001.fastq"
CONDITION='case_'
RENAME=$CONDITION$line
READ1RENAME="${RENAME}_R1_001.fastq"
READ2RENAME="${RENAME}_R2_001.fastq"
mv $READ1 $READ1RENAME
mv $READ2 $READ2RENAME

done < ~/microbiome_module/training_exercise/resources/patient_case.txt
```


***

*Creating symbolic links to the raw data*

Before we dive into our 16S analysis, it is good practice to think twice about working directly with the raw sequencing data. Instead of copying over all the files to our new directory and saving the raw files as backup, we can use symbolic links. Symbolic links are special file types that do not contain data themselves, but point to the raw sequencing data, much like an “alias” or “shortcut” does on a PC. Once a symbolic link is made, you can perform all the standard LINUX operations on it without working about the raw data.

We will use the “ln -s” command to make symbolic links. The ln -s command has the following format: 

```{bash}
ln -s raw_file symbolic_link_name
```

*Let's make a new directory for your analysis called "16S_analysis." Then, in the new 16S_analysis directory, using the ln -s command, create symbolic links to the raw data*

We can use the same name for the symbolic link as the original file so everything lines up.

```{bash}
# navigate to the training exercise directory
cd ~/microbiome_module/training_exercise

# make a new directory called "16S_analysis" and move into it
mkdir 16S_analysis
cd 16S_analysis

# test that ln -s works with just one file: case_G35391_R1_001.fastq
ln -s ~/microbiome_module/training_exercise/fastqs/case_G35391_R1_001.fastq case_G35391_R1_001.fastq

# use ls to look at the contents of the 16S_analysis directory and check that the link was made.
ls
```

*Note that successful links turn aqua blue!*

Now, using a “for loop,” we'll make symbolic links to all fastq files in the “fastqs” directory. 

When we made a symbolic link for just one sample, we specified that the name of the linked file should have the same name as the original filename. *BUT, for our "for loop", we'll want the output filename to change depending on the input filename - it won't always be case_G35391_R1_001.fastq* 

We can use the function **basename** to extract  the original filenames so we can then assign those as the output names. We will use the variable "FASTQ" for the original filenames and "LINKNAME" for the symbolic link names.

```{bash}

# for each raw FASTQ file in the directory
for FASTQ in ~/microbiome_module/training_exercise/fastqs/*.fastq
do 
LINKNAME="$(basename ${FASTQ})"
ln -s $FASTQ $LINKNAME
done

# list what files you've created
ls

```

***
















### *C. Explore the sequencing data files*

***

Let's get going now on our analysis. Take a peak at one of the fastq files. Does it look how you expect? *Use head to look at the R1 reads for patient "case_G35391"*

```{bash}

# navigate to the 16S_analysis directory and show all the files
cd ~/microbiome_module/training_exercise/16S_analysis
ls

# look at the first 2 fastq reads for patient case_G35391

head -8 case_G35391_R1_001.fastq

```

For each patient, we have 1 fastq file, each of which contains many sequencing reads. Recall that a fastq file has a specific architecture that gives us a lot of information – 4 lines per sequencing read give us the DNA sequence and some quality information indicating how confident we are about the assignment of each nucleotide base.

*Line 1: begins with an “@” character and contains a unique sequence identifier
*Line 2: the raw sequence letters
*Line 3: begins with a “+” character
*Line 4: quality score corresponding to each sequence letter in Line 2.

***


How many reads are there per patient sample? *Use wc to find the number of sequencing reads in the file “G35406_R1_001.fastq.” Hint: each read occupies 4 lines.*

```{bash}
# count the number of lines in the file case_G35391_R1_001.fastq
wc -l case_G35391_R1_001.fastq
```

If we divide the result by 4 we can see that there are 530388/4 = 132597 reads in that particular file.

*Choose another file to look at. Does it matter if they have different numbers of reads?*

```{bash}
# count the number of lines in the file case_G35955_R1_001.fastq
wc -l case_G35955_R1_001.fastq
```


***




** ZOOM OUT TO OVERVIEW **






### *D. Combining paired end reads into contiguous sequences in fasta format*


As we discussed in the lecture, there are advantages to sequencing a single 16S gene amplicon with more than 1 read. The DIABIMMUNE data are paired-end sequences of V4 region of the gene generated using Illumina HiSeq 2500 Platform.

Illumina paired-end sequencing uses 2 reads in opposing directions to cover (i) more of the target sequence (in our case, the 16S variable region of interest) with (ii) higher quality. *What do you think “R1” and “R2” stand for in our fastq filenames?*

For every 16S sequence we have a matching read 1 and read 2 stored in separate fastq files. At this point, we’d like to align the two paired reads and combine them into one contiguous, high quality fastq file. We will be using a program called **“FLASh.”**

**Figure 2: Paired end reads increase the length and quality of the assembled read** 
![](~/microbiome_module/training_exercise/scripts/Figure_2.png)

***

We can run **FLASh**. The usage of flash is as follows:

```{bash}
# use flash to align read_1 and read_2 and output an alignment file output_name
path/to/flash [read_1.fastq] [read_2.fastq] -o [output_name]

```

*First, run flash for just 1 sample: case_G35391_R1_001.fastq. Use “ls” to look in your 16S_analysis directory again. What new files have been generated?*

NOTE: You can tell “ls” to just look only at files that begin with “case_G35391” since at this point, those are the only files we’ve been working with.

```{bash}

# use flash to align read_1 and read_2 and output an alignment file output_name
~/FLASH-1.2.11-Linux-x86_64/flash case_G35391_R1_001.fastq case_G35391_R2_001.fastq -o case_G35391

# use ls to look only at files starting with A_control. What new files have been created?
ls case_G35391*

```

case_G35391.extendedFrags.fastq contains your paired read 1 (aka Mate 1) and read 2 (aka Mate 2). *Has every single read 1 been paired with a read 2? Check to see how many reads were paired. What are some reasons why reads might not be paired? Hint: look at the flash output.* 

Recall that case_G35391_R1_001.fastq had 530388 lines, corresponding to 132597 reads. 

```{bash}
wc -l case_G35391.extendedFrags.fastq
```

497084/4 = 124271 fastq reads have been paired (124271/132597 = 93.7%)

***










***

At this point, we could proceed and run flash individually for all patient samples. 

OR, we could take advantage of our “for loop” experience to write a script that will iteratively progress through all the paired reads, running flash and assigning the patient sample name to the output. *Today we'll use a for loop approach to save time!*

Using the function **basename** again, our approach will be as follows:
*identify an R1 fastq filename
*remove the ".R1_sub.fastq" suffix to generate the prefix only
*generate the corresponding R2 fastq filename by appending the ".R2_sub.fastq" suffix instead
*use the filename prefix as the output file prefix (no suffix)

```{bash}
# for all R1 fastq files with variable name FASTQ1
# cut off the R1_001.fastq suffix and call the prefix the variable SAMPLENAME
# define the R2 fastq filename as FASTQ, or the SAMPLENAME plus the R2_001.fastq suffix
# run flash on FASTQ R1 and R2 and output the SAMPLENAME file

for FASTQ1 in *R1_001.fastq
do 
echo $FASTQ1
SAMPLENAME="$(basename ${FASTQ1%_R1_001.fastq})"
FASTQ2="${SAMPLENAME}_R2_001.fastq";
~/FLASH-1.2.11-Linux-x86_64/flash $FASTQ1 $FASTQ2 -o $SAMPLENAME
done

```


*Use ls to find all the .extendedFrags.fastq files. Do you have one file for each patient sample?*

```{bash}
# Use the wildcard with suffix extendedFrags.fastq to see matches for just that filetype
ls *.extendedFrags.fastq
```

***





***

Now that we’ve successfully combined our reads, we no longer need quality score information at every base. Let’s convert our .extendedFrags.fastq files to fasta files. Recall that fasta files are just 2 lines:

* Line 1: begins with a “>” character and contains a unique sequence identifier
* Line 2: the raw sequence letters

We can use a handy program called “fastq_to_fasta” to accomplish this goal. We run fastq_to_fasta in bash with the following format: 

```{bash}
fastq_to_fasta -i [fastq input filename] -o [fasta output filename]
```

*Convert our favorite case_G35391 patient combined read fastq file to a fasta file.* We can now see the case_G35391.fasta appears in our directory and the contents are indeed in fasta format.



```{bash}
# run the fastq_to_fasta program with -i [input filename] and -o [output filename]
/usr/local/bin/fastq_to_fasta -i case_G35391.extendedFrags.fastq -o case_G35391.fasta

# just look at what new files you've made for the case_G35391 patient using the wildcard
ls case_G35391*
```

*As we’ve done several times now, use a “for loop” to apply this fastq to fasta conversion to all the patient files. Again, use the basename command again along with some nifty code that removes the file type. Double check to make sure you have one fasta file per patient sample.*

**NOTE: This step will take a few minutes, so use this opportunity to stretch**

```{bash}
# for all files with the extendedFrags.fastq suffix, assigned variable FASTQ
# SAMPLEID is the prefix for each filename
# convert the fastq to a fasta file using the same filename prefix but a new .fasta suffix
for FASTQ in *extendedFrags.fastq
do
SAMPLEID="${FASTQ%.extendedFrags.fastq}"
fastq_to_fasta -i $FASTQ -o $SAMPLEID.fasta
done

# look at the new .fasta files you've generated using the wildcard
ls *.fasta
```

***





** ZOOM OUT TO OVERVIEW **




```{bash}
#if taking too long
cd ~/microbiome_module/training_exercise/example_files
cp case*.fasta ~/microbiome_module/training_exercise/16S_analysis
cp control*.fasta ~/microbiome_module/training_exercise/16S_analysis
cd ~/microbiome_module/training_exercise/16S_analysis
ls *.fasta
```






***

### *E. Generate list of unique, non-singleton 16S sequences*


***

To begin the process of building a catalog of all the organsisms we've found, we first want to pool fasta sequences across all patient samples so we can identify OTUs and assign taxonomy. A custom script called **“samples2fasta.sh”** will allow us to run this operation. We can use samples2fasta.sh as follows:

```{bash}
bash /path/to/samples2fasta.sh [directory with fasta files to be combined] [output file name]
```

*Run samples2fasta. Note that “.” is a useful character that means your “current working directory.”*

```{bash}
# run samples2fasta in current directory (.) and with output filename "all_samples.fasta"
bash ~/microbiome_module/training_exercise/scripts/samples2fasta.sh . all_samples.fasta
```

*How many total sequence were pooled? Recall that every fasta file starts with the “>” character.*

```{bash}
# determine the number of fasta files in the all_samples.fasta document with grep and wc
grep ">" all_samples.fasta | wc -l
```

***













***

Since we are interested in building a catalog of unique bacterial taxons, let’s remove identical (aka duplicate) sequences, generating a list of just unique 16S sequences. 

We can use the program **“usearch”** to accomplish this goal as it has a built in function, **fastx_uniques**, for performing this exact operation. When used with the -sizeout flag, fastx_uniques will report how many times each unique sequence was found in the dataset.


We can run fastx uniques with the following format:

```{bash}
path/to/usearch -fastx_uniques [input filename] -fastaout [output filename] -sizeout
```

*Find unique 16S sequences in "all_samples.fasta" and output the results to "all_unique_seqs.fasta"*

```{bash}

# run the fastx_uniques function in usearch
/usr/bin/usearch10.0-2.240_i86linux32 -fastx_uniques all_samples.fasta -fastaout all_unique_seqs.fasta -sizeout

```

*Look at the ouput file all_unique_seqs.fasta. See how a new parameter “size=xxx” has been added to the sequence identifier for each sequence in the fasta file according to how many times that unique sequence was found.*

```{bash}
# use "head" to take a look at the contents of "all_unique_seqs.fasta"
head all_unique_seqs.fasta
```

*Also, look at the fastx_unique output. How many singletons were found? Do you think that singleton 16S sequences are likely to represent their own unique bacterial taxons? Are singletons more likely to have arisen due to sequencing errors?*







***

Since bacterial taxons present in the original patient sample at a detectable amount would likely yield multiple 16S sequences in the dataset, it is typical to remove singletons. The usearch function **sortbysize** will sort the unique sequences according to their frequency of occurrence, and **-minsize** will remove sequences that were only found a given number of times. 

The format for usearch sortbysize is as follows:

```{bash}
path/to/usearch -sortbysize [input filename] -fastaout [output filename] -minsize [minimum number of unique sequence occurances]

```

*Run sortbysize for our all_unique_seqs.fasta file, with output filename “all_unique_seqs_sorted.fasta” and  minsize set to 2 to remove singletons.*

```{bash}

# remove singletons and sort our unique 16S sequences
/usr/bin/usearch10.0-2.240_i86linux32 -sortbysize all_unique_seqs.fasta -fastaout all_unique_seqs_sorted.fasta -minsize 2
```

*How many non-singleton, unique sequences remain?*

***






** ZOOM OUT TO OVERVIEW **







### *F.	Cluster unique 16S sequences into operational taxonomic units and determine OTU abundance*

***

As discussed in the “Introduction to sequencing microbiota” lecture, we assume that sequence differences in the variable regions of the 16S gene reflect taxonomic (ideally species-level) difference between different microbes. As we discussed, identifying species is a balance between permitting some strain-level variation, while also maintaining a degree of separation between distinct 16S sequences. Here, we will assume that sequences with <3% sequence variation are of the same bacterial taxon, while sequences with >3% sequence variation are of different bacterial taxons.

Each cluster or grouping of 16S sequences with <3% variation is called an “operation taxonomic unit,” or OTU. Later in the module, we will attempt to make a one to one mapping of OTUs to the names of known bacterial species. 

***

Another tool in **usearch**, the **“-cluster_otus”** option, will allow us to generate OTUs from our non-singleton unique 16S fasta sequences. The operation takes the format:

```{bash}
path/to/usearch -cluster_otus [input filename] -otus [output filename]
```

***

Up to this point, we have assumed that the 16S gene sequences assembled by flash are all representative of true 16S gene sequences. However, it is known that PCR amplification of the 16S gene prior to Illumina paired end sequencing can lead to the formation of mixed, or chimeric, 16S gene sequences. OTU clustering algorithms such as usearch are designed to remove these artifactual 16S sequences.

*Run the algorithm for our samples to generate the output file “otus.fasta”. How many OTUs were generated? Were any chimeras detected?*

```{bash}
# cluster otus for "all_unique_seqs_sorted.fasta"" with the output filename "otus.fasta"
/usr/bin/usearch10.0-2.240_i86linux32 -cluster_otus all_unique_seqs_sorted.fasta -otus otus.fasta
```

*Look at the file otus.fasta. Are the sequence identifiers particularly useful?*

***









***

Our file "otus.fasta" contains a single representative sequence, or “OTU sequence” for each OTU. Let’s relabel each OTU with a simple identifier (a single number) for easier reference moving forward. Use the python script “fasta_number.py” in the format:

```{bash}
python path/to/fasta_number.py [input fasta] OTU_ > [output fasta]
```

*Now run fasta_number.py for our "otus.fasta" file and name the resulting output file "otus_renamed.fasta"*

```{bash}
python ~/microbiome_module/training_exercise/scripts/fasta_number.py otus.fasta OTU_ > otus_renamed.fasta
```

*Look again at the file otus_renamed.fasta. Have the representative sequence identifiers changed?*

***








** ZOOM OUT TO OVERVIEW **








***

**Creating a table of OTU abundance**

Now we want to know how abundant is each taxon in individual patient samples? Are some OTUs abundant in healthy patients but not in patients with disease? Now lets revisit the *non-pooled* patient samples and look for how many times each patient read matches one OTU or another.

We will again use the usearch algorithm, this time with the “-usearch_global” flag with the all_samples.fasta input and the otus_renamed.fasta database. This command has the format:

```{bash}
path/to/usearch -usearch_global [sequences to match] -db [representative OTU sequences] -strand plus -id 0.97 -uc out_map.uc
```

*Now run usearch_global for the OTU sequences listed in "otus_renamed.fasta" and assign matches for all the sequences in "all_samples.fasta" with a stringency of 97% similarity. Output results to out_map.uc*

```{bash}
/usr/bin/usearch10.0-2.240_i86linux32 -usearch_global all_samples.fasta -db otus_renamed.fasta -strand plus -id 0.97 -uc otu_map.uc
```

*Take a peak at the output file*

```{bash}
head otu_map.uc
```

***

A count table that summarizes the number of times each OTU was found in a given patient sample would be very valuable. We can use the uc2otutab.py python script to produce the otu_matrix.tsv, an “OTU table” file that we will use to derive meaningful experimental insight. We can run uc2otutab.py according to the format:

```{bash}
python /path/to/uc2otutab.py [input map] > [output table name]
```

Run this script for our "otu_map.uc" file and call the output table "otu_matrix.tsv"

```{bash}
python ~/microbiome_module/training_exercise/scripts/uc2otutab.py otu_map.uc > otu_matrix.tsv
```

***













***

### *I.	Assign taxonomy to OTU sequences*

We now have a table of OTU abundance, and a representative sequence for each OTU, but what genus or species does each representative sequence represent? At this point, we have excellent databases of 16S gene sequences and corresponding taxonomic matches. One such reference database can be accessed via the “Ribosomal Database Project” (RDP) classifier tool.

The RDP classifier tool is written in java, so to run it, we will first call java and then specify the path to the program. Our output file will be named "otu_taxonomy_rdp_0.8.tsv" Set a "-c" confidence threshold of 0.8.

*Run the RDP classifer. As this tool is written in java, it's necessary to first run java then pass the location of the installed classifier*

```{bash}
java -Xmx1g -jar /usr/local/rdp_classifier_2.12/dist/classifier.jar classify -f filterbyconf -c 0.8 -o otu_taxonomy_rdp_0.8.tsv -h otu_taxonomy.hierachy otus_renamed.fasta
```

*Take a look at the taxonomy_rdp_0.8.tsv file. Which species does OTU_6 correspond to?*

```{bash}
head otu_taxonomy_rdp_0.8.tsv
```

Alternatively, we can always use the NCBI Blast tool to compare each OTU sequence against the NCBI database.

***






** ZOOM OUT TO OVERVIEW **









***

## Part 3: Visualizing 16S results

***






###*A. Review file import and plotting in R*


**SWITCH TO "CONSOLE" TO USE R**


In parts 1 and 2 we used the Linux command line to process 16S data and generate OTU tables and associated taxonomy. Now, we transition to using the statistical programming language R to further analyze and plot these data. To begin, we need to tell “R” where to look for our files. Use the “setwd()” command to set your working directory. The command is used as follows: 

```{r}
setwd(“path/to/working/directory”)
```

*Now let's import the results from our microbiome training exercise.*

```{r}
setwd("~/microbiome_module/training_exercise/16S_analysis")
```

***

Once we've told "R" where to look for our files, we can then use the read.table function to import the data we’ve already generated. Read.table can import data from different file types. By specifying a "tab" separator ("\t") as the character that distinguishes values in our table, we can import "tab separated values" (tsv) files. Or, to import "comma separated values“ (csv), specify that the value separator is a comma (","). row.names tells R where to look for the row names, and header = TRUE tells R that there are some cells in our table that do not contain values, but rather are data descriptors.

```{r}
# this is the general format to use
table_name <- read.table(“input_filename”,row.names=1,header=TRUE,sep= “\t”)
```

*Now, import the otu count matrix (“otu_matrix.tsv”) and the otu taxonomy table (“otu_taxonomy_rdp_0.8.tvsv”). Don't forget to use the full path to any files not in our current working directory*
 
```{r}
df_count <- read.table('otu_matrix.tsv', row.names=1, header=TRUE, sep='\t')
df_tax <- read.table('otu_taxonomy_rdp_0.8.tsv', row.names=1, header=TRUE, sep='\t')
df_meta <-read.table('~/microbiome_module/training_exercise/resources/metadata_last_collection.csv', header=TRUE, sep=',', row.names=1) 
```

The command “head” works in R just as it did for us on the Linux command line. Use head to take a look at the three imported tables.

```{r}
head(df_count)
head(df_tax)
head(df_meta)
```

There are handy packages, such as “phyloseq” specifically designed to produce beautiful 16S sequencing plots from our data, but our imported tables are in the wrong format. Now that we have our imported tables, we’ve got to reformat them as data frames. Use the custom script “prepare_data_frames.R” to reformat our data. 

```{r}
source("/home/ubuntu/microbiome_module/training_exercise/scripts/prepare_data_frames.R")
```

***













***

##*B.	Plot relative abundance for healthy v disease groups*

Once your data are contained within a “phyloseq” object, it is easy to generate sophisticated plots with relatively little effort.To plot a stacked bar chart of taxon abundance, use the “plot_bar” function in R.


```{r}
plot_bar(physeq_norm, fill="phylum")
plot_bar(physeq_norm, fill="class")
```

*What do you think the outcome of the experiment is? Do you see a difference between case and control groups?*

***







##*C.	Plot alpha diversity for case v control*

While individual taxa are of interest, many biologically relevant changes in the microbiome (for example dysbiosis) are reported at the community level. Statistics that summarize changes in microbiome community composition are therefore of interest.

Alpha diversity metrics represent measurements of microbiome diversity within an individual. For comparative purposes, it is possible to compare the alpha diversity of individual A with that of individual B. There are multiple different statistical methods for measuring alpha diversity.Phyloseq provides convenient access to many of these methods.

*Plot multiple measures of alpha diversity with phyloseq.*

```{r}
# plot alpha diversity for all statstical methods
plot_richness(physeq_norm)

# for better visualization, just choose one method
plot_richness(physeq_norm,measures = "Shannon")

# we can color data points according to the metadata
plot_richness(physeq_norm,measures = "Shannon",color = 'Gender')
```

***

[http://deneflab.github.io/MicrobeMiseq/demos/mothur_2_phyloseq.html]



***

##*D.	Plot beta diversity for case v control*

In contrast to alpha diversity, beta diversity metrics represent measurments of the extent to which the microbiome changes across individuals. 

To plot beta diversity, it is first necessary to generate a metric that summarizes how muchthe microbiome varies between every pair of individuals in a study. There are many such measures of similarity/dissimilarity. 

Pairwise comparison of similarity/dissimilarity between all sampels in a study can be used to generaet resemblance matrices, which can in turn, be used to plot visual summaries of beta diversity.

```{r}
# First generate a distance matrix in phyloseq
dist_norm <- phyloseq::distance(physeq_norm, method='bray')

# First it's necessary to perform the ordination
physeq_norm_ord <- ordinate(physeq_norm, method="PCoA", distance="bray")
```

*Now we can plot and use metadata to color our clusters*

```{r}
#plot beta diversity
plot_ordination(physeq_norm, physeq_norm_ord)

#list the available metadata types
metadata_types <- colnames(df_meta)
metadata_types

#plot beta diversity and color by metadata
plot_ordination(physeq_norm, physeq_norm_ord, color="T1D_Status")
plot_ordination(physeq_norm, physeq_norm_ord, color="Eggs")
plot_ordination(physeq_norm, physeq_norm_ord, color="Delivery_Route")

#plot beta diversity and color by metadata
plot_ordination(physeq_norm, physeq_norm_ord, color="T1D_Status",shape="BF_Exclusive_Positive")+geom_point(size = 4)
```

***




** ZOOM OUT TO OVERVIEW **




